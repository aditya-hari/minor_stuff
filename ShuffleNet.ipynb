{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from keras import backend as K\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.models import Model\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.layers import Activation, Add, Concatenate, GlobalAveragePooling2D,GlobalMaxPooling2D, Input, Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, BatchNormalization, Lambda\n",
    "from keras.applications.mobilenet import DepthwiseConv2D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShuffleNet(include_top=True, input_tensor=None, scale_factor=1.0, pooling='max',\n",
    "               input_shape=(224,224,3), groups=1, load_model=None, num_shuffle_units=[3, 7, 3],\n",
    "               bottleneck_ratio=0.25, classes=4):\n",
    "    \"\"\"\n",
    "    ShuffleNet implementation for Keras 2\n",
    "    ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices\n",
    "    Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun\n",
    "    https://arxiv.org/pdf/1707.01083.pdf\n",
    "    Note that only TensorFlow is supported for now, therefore it only works\n",
    "    with the data format `image_data_format='channels_last'` in your Keras\n",
    "    config at `~/.keras/keras.json`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    include_top: bool(True)\n",
    "         whether to include the fully-connected layer at the top of the network.\n",
    "    input_tensor:\n",
    "        optional Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model.\n",
    "    scale_factor:\n",
    "        scales the number of output channels\n",
    "    input_shape:\n",
    "    pooling:\n",
    "        Optional pooling mode for feature extraction\n",
    "        when `include_top` is `False`.\n",
    "        - `None` means that the output of the model\n",
    "            will be the 4D tensor output of the\n",
    "            last convolutional layer.\n",
    "        - `avg` means that global average pooling\n",
    "            will be applied to the output of the\n",
    "            last convolutional layer, and thus\n",
    "            the output of the model will be a\n",
    "            2D tensor.\n",
    "        - `max` means that global max pooling will\n",
    "            be applied.\n",
    "    groups: int\n",
    "        number of groups per channel\n",
    "    num_shuffle_units: list([3,7,3])\n",
    "        number of stages (list length) and the number of shufflenet units in a\n",
    "        stage beginning with stage 2 because stage 1 is fixed\n",
    "        e.g. idx 0 contains 3 + 1 (first shuffle unit in each stage differs) shufflenet units for stage 2\n",
    "        idx 1 contains 7 + 1 Shufflenet Units for stage 3 and\n",
    "        idx 2 contains 3 + 1 Shufflenet Units\n",
    "    bottleneck_ratio:\n",
    "        bottleneck ratio implies the ratio of bottleneck channels to output channels.\n",
    "        For example, bottleneck ratio = 1 : 4 means the output feature map is 4 times\n",
    "        the width of the bottleneck feature map.\n",
    "    classes: int(1000)\n",
    "        number of classes to predict\n",
    "    Returns\n",
    "    -------\n",
    "        A Keras model instance\n",
    "    References\n",
    "    ----------\n",
    "    - [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices]\n",
    "      (http://www.arxiv.org/pdf/1707.01083.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    if K.backend() != 'tensorflow':\n",
    "        raise RuntimeError('Only TensorFlow backend is currently supported, '\n",
    "                           'as other backends do not support ')\n",
    "\n",
    "    name = \"ShuffleNet_%.2gX_g%d_br_%.2g_%s\" % (scale_factor, groups, bottleneck_ratio, \"\".join([str(x) for x in num_shuffle_units]))\n",
    "\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=28,\n",
    "                                      require_flatten=include_top,\n",
    "                                      data_format=K.image_data_format())\n",
    "\n",
    "    out_dim_stage_two = {1: 144, 2: 200, 3: 240, 4: 272, 8: 384}\n",
    "    if groups not in out_dim_stage_two:\n",
    "        raise ValueError(\"Invalid number of groups.\")\n",
    "\n",
    "    if pooling not in ['max','avg']:\n",
    "        raise ValueError(\"Invalid value for pooling.\")\n",
    "\n",
    "    if not (float(scale_factor) * 4).is_integer():\n",
    "        raise ValueError(\"Invalid value for scale_factor. Should be x over 4.\")\n",
    "\n",
    "    exp = np.insert(np.arange(0, len(num_shuffle_units), dtype=np.float32), 0, 0)\n",
    "    out_channels_in_stage = 2 ** exp\n",
    "    out_channels_in_stage *= out_dim_stage_two[groups]  # calculate output channels for each stage\n",
    "    out_channels_in_stage[0] = 24  # first stage has always 24 output channels\n",
    "    out_channels_in_stage *= scale_factor\n",
    "    out_channels_in_stage = out_channels_in_stage.astype(int)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    # create shufflenet architecture\n",
    "    x = Conv2D(filters=out_channels_in_stage[0], kernel_size=(3, 3), padding='same',\n",
    "               use_bias=False, strides=(2, 2), activation=\"relu\", name=\"conv1\")(img_input)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same', name=\"maxpool1\")(x)\n",
    "\n",
    "    # create stages containing shufflenet units beginning at stage 2\n",
    "    for stage in range(0, len(num_shuffle_units)):\n",
    "        repeat = num_shuffle_units[stage]\n",
    "        x = _block(x, out_channels_in_stage, repeat=repeat,\n",
    "                   bottleneck_ratio=bottleneck_ratio,\n",
    "                   groups=groups, stage=stage + 2)\n",
    "\n",
    "    if pooling == 'avg':\n",
    "        x = GlobalAveragePooling2D(name=\"global_pool\")(x)\n",
    "    elif pooling == 'max':\n",
    "        x = GlobalMaxPooling2D(name=\"global_pool\")(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = Dense(units=classes, name=\"fc\")(x)\n",
    "        x = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x, name=name)\n",
    "\n",
    "    if load_model is not None:\n",
    "        model.load_weights('', by_name=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _block(x, channel_map, bottleneck_ratio, repeat=1, groups=1, stage=1):\n",
    "    \"\"\"\n",
    "    creates a bottleneck block containing `repeat + 1` shuffle units\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "        Input tensor of with `channels_last` data format\n",
    "    channel_map: list\n",
    "        list containing the number of output channels for a stage\n",
    "    repeat: int(1)\n",
    "        number of repetitions for a shuffle unit with stride 1\n",
    "    groups: int(1)\n",
    "        number of groups per channel\n",
    "    bottleneck_ratio: float\n",
    "        bottleneck ratio implies the ratio of bottleneck channels to output channels.\n",
    "        For example, bottleneck ratio = 1 : 4 means the output feature map is 4 times\n",
    "        the width of the bottleneck feature map.\n",
    "    stage: int(1)\n",
    "        stage number\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    x = _shuffle_unit(x, in_channels=channel_map[stage - 2],\n",
    "                      out_channels=channel_map[stage - 1], strides=2,\n",
    "                      groups=groups, bottleneck_ratio=bottleneck_ratio,\n",
    "                      stage=stage, block=1)\n",
    "\n",
    "    for i in range(1, repeat + 1):\n",
    "        x = _shuffle_unit(x, in_channels=channel_map[stage - 1],\n",
    "                          out_channels=channel_map[stage - 1], strides=1,\n",
    "                          groups=groups, bottleneck_ratio=bottleneck_ratio,\n",
    "                          stage=stage, block=(i + 1))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def _shuffle_unit(inputs, in_channels, out_channels, groups, bottleneck_ratio, strides=2, stage=1, block=1):\n",
    "    \"\"\"\n",
    "    creates a shuffleunit\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs:\n",
    "        Input tensor of with `channels_last` data format\n",
    "    in_channels:\n",
    "        number of input channels\n",
    "    out_channels:\n",
    "        number of output channels\n",
    "    strides:\n",
    "        An integer or tuple/list of 2 integers,\n",
    "        specifying the strides of the convolution along the width and height.\n",
    "    groups: int(1)\n",
    "        number of groups per channel\n",
    "    bottleneck_ratio: float\n",
    "        bottleneck ratio implies the ratio of bottleneck channels to output channels.\n",
    "        For example, bottleneck ratio = 1 : 4 means the output feature map is 4 times\n",
    "        the width of the bottleneck feature map.\n",
    "    stage: int(1)\n",
    "        stage number\n",
    "    block: int(1)\n",
    "        block number\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = -1\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    prefix = 'stage%d/block%d' % (stage, block)\n",
    "\n",
    "    #if strides >= 2:\n",
    "        #out_channels -= in_channels\n",
    "\n",
    "    # default: 1/4 of the output channel of a ShuffleNet Unit\n",
    "    bottleneck_channels = int(out_channels * bottleneck_ratio)\n",
    "    groups = (1 if stage == 2 and block == 1 else groups)\n",
    "\n",
    "    x = _group_conv(inputs, in_channels, out_channels=bottleneck_channels,\n",
    "                    groups=(1 if stage == 2 and block == 1 else groups),\n",
    "                    name='%s/1x1_gconv_1' % prefix)\n",
    "    x = BatchNormalization(axis=bn_axis, name='%s/bn_gconv_1' % prefix)(x)\n",
    "    x = Activation('relu', name='%s/relu_gconv_1' % prefix)(x)\n",
    "\n",
    "    x = Lambda(channel_shuffle, arguments={'groups': groups}, name='%s/channel_shuffle' % prefix)(x)\n",
    "    x = DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", use_bias=False,\n",
    "                        strides=strides, name='%s/1x1_dwconv_1' % prefix)(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name='%s/bn_dwconv_1' % prefix)(x)\n",
    "\n",
    "    x = _group_conv(x, bottleneck_channels, out_channels=out_channels if strides == 1 else out_channels - in_channels,\n",
    "                    groups=groups, name='%s/1x1_gconv_2' % prefix)\n",
    "    x = BatchNormalization(axis=bn_axis, name='%s/bn_gconv_2' % prefix)(x)\n",
    "\n",
    "    if strides < 2:\n",
    "        ret = Add(name='%s/add' % prefix)([x, inputs])\n",
    "    else:\n",
    "        avg = AveragePooling2D(pool_size=3, strides=2, padding='same', name='%s/avg_pool' % prefix)(inputs)\n",
    "        ret = Concatenate(bn_axis, name='%s/concat' % prefix)([x, avg])\n",
    "\n",
    "    ret = Activation('relu', name='%s/relu_out' % prefix)(ret)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _group_conv(x, in_channels, out_channels, groups, kernel=1, stride=1, name=''):\n",
    "    \"\"\"\n",
    "    grouped convolution\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "        Input tensor of with `channels_last` data format\n",
    "    in_channels:\n",
    "        number of input channels\n",
    "    out_channels:\n",
    "        number of output channels\n",
    "    groups:\n",
    "        number of groups per channel\n",
    "    kernel: int(1)\n",
    "        An integer or tuple/list of 2 integers, specifying the\n",
    "        width and height of the 2D convolution window.\n",
    "        Can be a single integer to specify the same value for\n",
    "        all spatial dimensions.\n",
    "    stride: int(1)\n",
    "        An integer or tuple/list of 2 integers,\n",
    "        specifying the strides of the convolution along the width and height.\n",
    "        Can be a single integer to specify the same value for all spatial dimensions.\n",
    "    name: str\n",
    "        A string to specifies the layer name\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    if groups == 1:\n",
    "        return Conv2D(filters=out_channels, kernel_size=kernel, padding='same',\n",
    "                      use_bias=False, strides=stride, name=name)(x)\n",
    "\n",
    "    # number of intput channels per group\n",
    "    ig = in_channels // groups\n",
    "    group_list = []\n",
    "\n",
    "    assert out_channels % groups == 0\n",
    "\n",
    "    for i in range(groups):\n",
    "        offset = i * ig\n",
    "        group = Lambda(lambda z: z[:, :, :, offset: offset + ig], name='%s/g%d_slice' % (name, i))(x)\n",
    "        group_list.append(Conv2D(int(0.5 + out_channels / groups), kernel_size=kernel, strides=stride,\n",
    "                                 use_bias=False, padding='same', name='%s_/g%d' % (name, i))(group))\n",
    "    return Concatenate(name='%s/concat' % name)(group_list)\n",
    "\n",
    "\n",
    "def channel_shuffle(x, groups):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "        Input tensor of with `channels_last` data format\n",
    "    groups: int\n",
    "        number of groups per channel\n",
    "    Returns\n",
    "    -------\n",
    "        channel shuffled output tensor\n",
    "    Examples\n",
    "    --------\n",
    "    Example for a 1D Array with 3 groups\n",
    "    >>> d = np.array([0,1,2,3,4,5,6,7,8])\n",
    "    >>> x = np.reshape(d, (3,3))\n",
    "    >>> x = np.transpose(x, [1,0])\n",
    "    >>> x = np.reshape(x, (9,))\n",
    "    '[0 1 2 3 4 5 6 7 8] --> [0 3 6 1 4 7 2 5 8]'\n",
    "    \"\"\"\n",
    "    height, width, in_channels = x.shape.as_list()[1:]\n",
    "    channels_per_group = in_channels // groups\n",
    "\n",
    "    x = K.reshape(x, [-1, height, width, groups, channels_per_group])\n",
    "    x = K.permute_dimensions(x, (0, 1, 2, 4, 3))  # transpose\n",
    "    x = K.reshape(x, [-1, height, width, in_channels])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/siddhant/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:63: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/siddhant/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:492: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/siddhant/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3630: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/siddhant/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3458: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/siddhant/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3462: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/siddhant/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1208: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model = ShuffleNet(classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3400 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('/home/siddhant/Datasets/Minor/images/cropped/train/', #Replace with path to train set\n",
    "                                                 target_size = (224 , 224),\n",
    "                                                 batch_size = 64,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('/home/siddhant/Datasets/Minor/images/cropped/test', #Replace with path to test set\n",
    "                                            target_size = (224 , 224),\n",
    "                                            batch_size = 64,\n",
    "                                            class_mode = 'categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/siddhant/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/siddhant/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2880: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/siddhant/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2884: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "import keras\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=0.00001, momentum=0.9)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy' , metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classweights = {0:1.0 , 1:1.1 , 2:1.1, 3:9.1} #Custom class weights to account for unbalanced dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "FILE_NAME = 'shuffleNet'\n",
    "tboard = TensorBoard(log_dir=f'logs/{FILE_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/siddhant/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/siddhant/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:953: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/siddhant/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:675: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/50\n",
      "54/53 [==============================] - 49s 910ms/step - loss: 9.4024 - acc: 0.2520 - val_loss: 1.3954 - val_acc: 0.2500\n",
      "Epoch 2/50\n",
      "54/53 [==============================] - 39s 722ms/step - loss: 1.9678 - acc: 0.4347 - val_loss: 1.4108 - val_acc: 0.2500\n",
      "Epoch 3/50\n",
      "54/53 [==============================] - 45s 835ms/step - loss: 1.7172 - acc: 0.4781 - val_loss: 1.4394 - val_acc: 0.2500\n",
      "Epoch 4/50\n",
      "54/53 [==============================] - 47s 872ms/step - loss: 1.3121 - acc: 0.5841 - val_loss: 1.4884 - val_acc: 0.2250\n",
      "Epoch 5/50\n",
      "54/53 [==============================] - 51s 943ms/step - loss: 1.0988 - acc: 0.6609 - val_loss: 1.5608 - val_acc: 0.2400\n",
      "Epoch 6/50\n",
      "54/53 [==============================] - 48s 890ms/step - loss: 0.9920 - acc: 0.6929 - val_loss: 1.6128 - val_acc: 0.2475\n",
      "Epoch 7/50\n",
      "54/53 [==============================] - 48s 888ms/step - loss: 0.8358 - acc: 0.7321 - val_loss: 1.7545 - val_acc: 0.2500\n",
      "Epoch 8/50\n",
      "54/53 [==============================] - 50s 925ms/step - loss: 0.7464 - acc: 0.7751 - val_loss: 1.8799 - val_acc: 0.2900\n",
      "Epoch 9/50\n",
      "54/53 [==============================] - 50s 928ms/step - loss: 0.6608 - acc: 0.7871 - val_loss: 1.6478 - val_acc: 0.4625\n",
      "Epoch 10/50\n",
      "54/53 [==============================] - 51s 946ms/step - loss: 0.5800 - acc: 0.8274 - val_loss: 1.1776 - val_acc: 0.4800\n",
      "Epoch 11/50\n",
      "54/53 [==============================] - 55s 1s/step - loss: 0.5528 - acc: 0.8282 - val_loss: 0.9388 - val_acc: 0.5875\n",
      "Epoch 12/50\n",
      "54/53 [==============================] - 51s 949ms/step - loss: 0.4834 - acc: 0.8526 - val_loss: 0.7520 - val_acc: 0.6475\n",
      "Epoch 13/50\n",
      "54/53 [==============================] - 56s 1s/step - loss: 0.4837 - acc: 0.8586 - val_loss: 0.4158 - val_acc: 0.8275\n",
      "Epoch 14/50\n",
      "54/53 [==============================] - 52s 960ms/step - loss: 0.4483 - acc: 0.8639 - val_loss: 0.3722 - val_acc: 0.8550\n",
      "Epoch 15/50\n",
      "54/53 [==============================] - 49s 912ms/step - loss: 0.4270 - acc: 0.8793 - val_loss: 0.2760 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      "54/53 [==============================] - 52s 962ms/step - loss: 0.3750 - acc: 0.8915 - val_loss: 0.2299 - val_acc: 0.9250\n",
      "Epoch 17/50\n",
      "54/53 [==============================] - 58s 1s/step - loss: 0.3646 - acc: 0.8833 - val_loss: 0.1922 - val_acc: 0.9400\n",
      "Epoch 18/50\n",
      "54/53 [==============================] - 52s 964ms/step - loss: 0.2896 - acc: 0.9171 - val_loss: 0.2066 - val_acc: 0.9275\n",
      "Epoch 19/50\n",
      "54/53 [==============================] - 55s 1s/step - loss: 0.2844 - acc: 0.9153 - val_loss: 0.2520 - val_acc: 0.9025\n",
      "Epoch 20/50\n",
      "54/53 [==============================] - 56s 1s/step - loss: 0.2620 - acc: 0.9249 - val_loss: 0.2896 - val_acc: 0.8825\n",
      "Epoch 21/50\n",
      "54/53 [==============================] - 55s 1s/step - loss: 0.2620 - acc: 0.9274 - val_loss: 0.2535 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      "54/53 [==============================] - 56s 1s/step - loss: 0.2360 - acc: 0.9359 - val_loss: 0.1674 - val_acc: 0.9525\n",
      "Epoch 23/50\n",
      "54/53 [==============================] - 56s 1s/step - loss: 0.2280 - acc: 0.9358 - val_loss: 0.2032 - val_acc: 0.9375\n",
      "Epoch 24/50\n",
      "54/53 [==============================] - 57s 1s/step - loss: 0.2405 - acc: 0.9330 - val_loss: 0.1570 - val_acc: 0.9525\n",
      "Epoch 25/50\n",
      "54/53 [==============================] - 52s 972ms/step - loss: 0.2025 - acc: 0.9408 - val_loss: 0.1832 - val_acc: 0.9400\n",
      "Epoch 26/50\n",
      "54/53 [==============================] - 57s 1s/step - loss: 0.2108 - acc: 0.9478 - val_loss: 0.2004 - val_acc: 0.9275\n",
      "Epoch 27/50\n",
      "54/53 [==============================] - 59s 1s/step - loss: 0.2151 - acc: 0.9342 - val_loss: 0.1563 - val_acc: 0.9525\n",
      "Epoch 28/50\n",
      "54/53 [==============================] - 59s 1s/step - loss: 0.2536 - acc: 0.9318 - val_loss: 1.2572 - val_acc: 0.6075\n",
      "Epoch 29/50\n",
      "54/53 [==============================] - 57s 1s/step - loss: 0.2652 - acc: 0.9257 - val_loss: 0.6480 - val_acc: 0.7375\n",
      "Epoch 30/50\n",
      "54/53 [==============================] - 57s 1s/step - loss: 0.2276 - acc: 0.9328 - val_loss: 0.3907 - val_acc: 0.8375\n",
      "Epoch 31/50\n",
      "54/53 [==============================] - 57s 1s/step - loss: 0.2044 - acc: 0.9434 - val_loss: 0.2122 - val_acc: 0.9100\n",
      "Epoch 32/50\n",
      "54/53 [==============================] - 56s 1s/step - loss: 0.2050 - acc: 0.9426 - val_loss: 0.1604 - val_acc: 0.9325\n",
      "Epoch 33/50\n",
      "54/53 [==============================] - 54s 1s/step - loss: 0.1973 - acc: 0.9479 - val_loss: 0.1548 - val_acc: 0.9450\n",
      "Epoch 34/50\n",
      "54/53 [==============================] - 60s 1s/step - loss: 0.1665 - acc: 0.9493 - val_loss: 0.2732 - val_acc: 0.8850\n",
      "Epoch 35/50\n",
      "54/53 [==============================] - 55s 1s/step - loss: 0.1620 - acc: 0.9528 - val_loss: 0.1016 - val_acc: 0.9550\n",
      "Epoch 36/50\n",
      "54/53 [==============================] - 56s 1s/step - loss: 0.1441 - acc: 0.9612 - val_loss: 0.1213 - val_acc: 0.9475\n",
      "Epoch 37/50\n",
      "54/53 [==============================] - 56s 1s/step - loss: 0.1708 - acc: 0.9562 - val_loss: 0.1488 - val_acc: 0.9425\n",
      "Epoch 38/50\n",
      "54/53 [==============================] - 57s 1s/step - loss: 0.2530 - acc: 0.9418 - val_loss: 0.2695 - val_acc: 0.8900\n",
      "Epoch 39/50\n",
      "54/53 [==============================] - 57s 1s/step - loss: 0.2330 - acc: 0.9303 - val_loss: 0.7715 - val_acc: 0.7550\n",
      "Epoch 40/50\n",
      "54/53 [==============================] - 63s 1s/step - loss: 0.1729 - acc: 0.9550 - val_loss: 0.2518 - val_acc: 0.9000\n",
      "Epoch 41/50\n",
      "54/53 [==============================] - 53s 984ms/step - loss: 0.1795 - acc: 0.9501 - val_loss: 0.2438 - val_acc: 0.9175\n",
      "Epoch 42/50\n",
      "54/53 [==============================] - 55s 1s/step - loss: 0.1472 - acc: 0.9576 - val_loss: 0.1095 - val_acc: 0.9625\n",
      "Epoch 43/50\n",
      "54/53 [==============================] - 57s 1s/step - loss: 0.1551 - acc: 0.9615 - val_loss: 0.1331 - val_acc: 0.9675\n",
      "Epoch 44/50\n",
      "54/53 [==============================] - 60s 1s/step - loss: 0.1311 - acc: 0.9676 - val_loss: 0.1342 - val_acc: 0.9600\n",
      "Epoch 45/50\n",
      "54/53 [==============================] - 53s 987ms/step - loss: 0.1259 - acc: 0.9634 - val_loss: 0.1200 - val_acc: 0.9675\n",
      "Epoch 46/50\n",
      "54/53 [==============================] - 57s 1s/step - loss: 0.1170 - acc: 0.9716 - val_loss: 0.1474 - val_acc: 0.9550\n",
      "Epoch 47/50\n",
      "54/53 [==============================] - 59s 1s/step - loss: 0.1044 - acc: 0.9760 - val_loss: 0.1506 - val_acc: 0.9475\n",
      "Epoch 48/50\n",
      "54/53 [==============================] - 56s 1s/step - loss: 0.1273 - acc: 0.9605 - val_loss: 0.1510 - val_acc: 0.9375\n",
      "Epoch 49/50\n",
      "54/53 [==============================] - 40s 732ms/step - loss: 0.1292 - acc: 0.9660 - val_loss: 0.1629 - val_acc: 0.9475\n",
      "Epoch 50/50\n",
      "54/53 [==============================] - 39s 715ms/step - loss: 0.1109 - acc: 0.9677 - val_loss: 0.1241 - val_acc: 0.9575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f945b8a71d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(training_set, epochs = 50, validation_data = test_set, \n",
    "          validation_steps=1,\n",
    "          steps_per_epoch=training_set.samples//training_set.batch_size, class_weight=classweights, callbacks=[tboard]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 112, 112, 24) 648         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "maxpool1 (MaxPooling2D)         (None, 56, 56, 24)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/1x1_gconv_1 (Conv (None, 56, 56, 36)   864         maxpool1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/bn_gconv_1 (Batch (None, 56, 56, 36)   144         stage2/block1/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/relu_gconv_1 (Act (None, 56, 56, 36)   0           stage2/block1/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/channel_shuffle ( (None, 56, 56, 36)   0           stage2/block1/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/1x1_dwconv_1 (Dep (None, 28, 28, 36)   324         stage2/block1/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/bn_dwconv_1 (Batc (None, 28, 28, 36)   144         stage2/block1/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/1x1_gconv_2 (Conv (None, 28, 28, 120)  4320        stage2/block1/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/bn_gconv_2 (Batch (None, 28, 28, 120)  480         stage2/block1/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/avg_pool (Average (None, 28, 28, 24)   0           maxpool1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/concat (Concatena (None, 28, 28, 144)  0           stage2/block1/bn_gconv_2[0][0]   \n",
      "                                                                 stage2/block1/avg_pool[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/relu_out (Activat (None, 28, 28, 144)  0           stage2/block1/concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_1 (Conv (None, 28, 28, 36)   5184        stage2/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/bn_gconv_1 (Batch (None, 28, 28, 36)   144         stage2/block2/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/relu_gconv_1 (Act (None, 28, 28, 36)   0           stage2/block2/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/channel_shuffle ( (None, 28, 28, 36)   0           stage2/block2/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_dwconv_1 (Dep (None, 28, 28, 36)   324         stage2/block2/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/bn_dwconv_1 (Batc (None, 28, 28, 36)   144         stage2/block2/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_2 (Conv (None, 28, 28, 144)  5184        stage2/block2/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/bn_gconv_2 (Batch (None, 28, 28, 144)  576         stage2/block2/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/add (Add)         (None, 28, 28, 144)  0           stage2/block2/bn_gconv_2[0][0]   \n",
      "                                                                 stage2/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/relu_out (Activat (None, 28, 28, 144)  0           stage2/block2/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_1 (Conv (None, 28, 28, 36)   5184        stage2/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/bn_gconv_1 (Batch (None, 28, 28, 36)   144         stage2/block3/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/relu_gconv_1 (Act (None, 28, 28, 36)   0           stage2/block3/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/channel_shuffle ( (None, 28, 28, 36)   0           stage2/block3/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_dwconv_1 (Dep (None, 28, 28, 36)   324         stage2/block3/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/bn_dwconv_1 (Batc (None, 28, 28, 36)   144         stage2/block3/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_2 (Conv (None, 28, 28, 144)  5184        stage2/block3/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/bn_gconv_2 (Batch (None, 28, 28, 144)  576         stage2/block3/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/add (Add)         (None, 28, 28, 144)  0           stage2/block3/bn_gconv_2[0][0]   \n",
      "                                                                 stage2/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/relu_out (Activat (None, 28, 28, 144)  0           stage2/block3/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_1 (Conv (None, 28, 28, 36)   5184        stage2/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/bn_gconv_1 (Batch (None, 28, 28, 36)   144         stage2/block4/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/relu_gconv_1 (Act (None, 28, 28, 36)   0           stage2/block4/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/channel_shuffle ( (None, 28, 28, 36)   0           stage2/block4/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_dwconv_1 (Dep (None, 28, 28, 36)   324         stage2/block4/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/bn_dwconv_1 (Batc (None, 28, 28, 36)   144         stage2/block4/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_2 (Conv (None, 28, 28, 144)  5184        stage2/block4/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/bn_gconv_2 (Batch (None, 28, 28, 144)  576         stage2/block4/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/add (Add)         (None, 28, 28, 144)  0           stage2/block4/bn_gconv_2[0][0]   \n",
      "                                                                 stage2/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/relu_out (Activat (None, 28, 28, 144)  0           stage2/block4/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_1 (Conv (None, 28, 28, 72)   10368       stage2/block4/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/bn_gconv_1 (Batch (None, 28, 28, 72)   288         stage3/block1/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/relu_gconv_1 (Act (None, 28, 28, 72)   0           stage3/block1/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/channel_shuffle ( (None, 28, 28, 72)   0           stage3/block1/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_dwconv_1 (Dep (None, 14, 14, 72)   648         stage3/block1/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/bn_dwconv_1 (Batc (None, 14, 14, 72)   288         stage3/block1/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_2 (Conv (None, 14, 14, 144)  10368       stage3/block1/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/bn_gconv_2 (Batch (None, 14, 14, 144)  576         stage3/block1/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/avg_pool (Average (None, 14, 14, 144)  0           stage2/block4/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/concat (Concatena (None, 14, 14, 288)  0           stage3/block1/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block1/avg_pool[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/relu_out (Activat (None, 14, 14, 288)  0           stage3/block1/concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_1 (Conv (None, 14, 14, 72)   20736       stage3/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/bn_gconv_1 (Batch (None, 14, 14, 72)   288         stage3/block2/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/relu_gconv_1 (Act (None, 14, 14, 72)   0           stage3/block2/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/channel_shuffle ( (None, 14, 14, 72)   0           stage3/block2/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_dwconv_1 (Dep (None, 14, 14, 72)   648         stage3/block2/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/bn_dwconv_1 (Batc (None, 14, 14, 72)   288         stage3/block2/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_2 (Conv (None, 14, 14, 288)  20736       stage3/block2/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/bn_gconv_2 (Batch (None, 14, 14, 288)  1152        stage3/block2/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/add (Add)         (None, 14, 14, 288)  0           stage3/block2/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/relu_out (Activat (None, 14, 14, 288)  0           stage3/block2/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_1 (Conv (None, 14, 14, 72)   20736       stage3/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/bn_gconv_1 (Batch (None, 14, 14, 72)   288         stage3/block3/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/relu_gconv_1 (Act (None, 14, 14, 72)   0           stage3/block3/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/channel_shuffle ( (None, 14, 14, 72)   0           stage3/block3/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_dwconv_1 (Dep (None, 14, 14, 72)   648         stage3/block3/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/bn_dwconv_1 (Batc (None, 14, 14, 72)   288         stage3/block3/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_2 (Conv (None, 14, 14, 288)  20736       stage3/block3/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/bn_gconv_2 (Batch (None, 14, 14, 288)  1152        stage3/block3/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/add (Add)         (None, 14, 14, 288)  0           stage3/block3/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/relu_out (Activat (None, 14, 14, 288)  0           stage3/block3/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_1 (Conv (None, 14, 14, 72)   20736       stage3/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/bn_gconv_1 (Batch (None, 14, 14, 72)   288         stage3/block4/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/relu_gconv_1 (Act (None, 14, 14, 72)   0           stage3/block4/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/channel_shuffle ( (None, 14, 14, 72)   0           stage3/block4/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_dwconv_1 (Dep (None, 14, 14, 72)   648         stage3/block4/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/bn_dwconv_1 (Batc (None, 14, 14, 72)   288         stage3/block4/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_2 (Conv (None, 14, 14, 288)  20736       stage3/block4/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/bn_gconv_2 (Batch (None, 14, 14, 288)  1152        stage3/block4/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/add (Add)         (None, 14, 14, 288)  0           stage3/block4/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/relu_out (Activat (None, 14, 14, 288)  0           stage3/block4/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_1 (Conv (None, 14, 14, 72)   20736       stage3/block4/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/bn_gconv_1 (Batch (None, 14, 14, 72)   288         stage3/block5/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/relu_gconv_1 (Act (None, 14, 14, 72)   0           stage3/block5/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/channel_shuffle ( (None, 14, 14, 72)   0           stage3/block5/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_dwconv_1 (Dep (None, 14, 14, 72)   648         stage3/block5/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/bn_dwconv_1 (Batc (None, 14, 14, 72)   288         stage3/block5/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_2 (Conv (None, 14, 14, 288)  20736       stage3/block5/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/bn_gconv_2 (Batch (None, 14, 14, 288)  1152        stage3/block5/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/add (Add)         (None, 14, 14, 288)  0           stage3/block5/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block4/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/relu_out (Activat (None, 14, 14, 288)  0           stage3/block5/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_1 (Conv (None, 14, 14, 72)   20736       stage3/block5/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/bn_gconv_1 (Batch (None, 14, 14, 72)   288         stage3/block6/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/relu_gconv_1 (Act (None, 14, 14, 72)   0           stage3/block6/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/channel_shuffle ( (None, 14, 14, 72)   0           stage3/block6/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_dwconv_1 (Dep (None, 14, 14, 72)   648         stage3/block6/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/bn_dwconv_1 (Batc (None, 14, 14, 72)   288         stage3/block6/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_2 (Conv (None, 14, 14, 288)  20736       stage3/block6/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/bn_gconv_2 (Batch (None, 14, 14, 288)  1152        stage3/block6/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/add (Add)         (None, 14, 14, 288)  0           stage3/block6/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block5/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/relu_out (Activat (None, 14, 14, 288)  0           stage3/block6/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_1 (Conv (None, 14, 14, 72)   20736       stage3/block6/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/bn_gconv_1 (Batch (None, 14, 14, 72)   288         stage3/block7/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/relu_gconv_1 (Act (None, 14, 14, 72)   0           stage3/block7/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/channel_shuffle ( (None, 14, 14, 72)   0           stage3/block7/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_dwconv_1 (Dep (None, 14, 14, 72)   648         stage3/block7/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/bn_dwconv_1 (Batc (None, 14, 14, 72)   288         stage3/block7/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_2 (Conv (None, 14, 14, 288)  20736       stage3/block7/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/bn_gconv_2 (Batch (None, 14, 14, 288)  1152        stage3/block7/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/add (Add)         (None, 14, 14, 288)  0           stage3/block7/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block6/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/relu_out (Activat (None, 14, 14, 288)  0           stage3/block7/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_1 (Conv (None, 14, 14, 72)   20736       stage3/block7/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/bn_gconv_1 (Batch (None, 14, 14, 72)   288         stage3/block8/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/relu_gconv_1 (Act (None, 14, 14, 72)   0           stage3/block8/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/channel_shuffle ( (None, 14, 14, 72)   0           stage3/block8/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_dwconv_1 (Dep (None, 14, 14, 72)   648         stage3/block8/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/bn_dwconv_1 (Batc (None, 14, 14, 72)   288         stage3/block8/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_2 (Conv (None, 14, 14, 288)  20736       stage3/block8/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/bn_gconv_2 (Batch (None, 14, 14, 288)  1152        stage3/block8/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/add (Add)         (None, 14, 14, 288)  0           stage3/block8/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block7/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/relu_out (Activat (None, 14, 14, 288)  0           stage3/block8/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_1 (Conv (None, 14, 14, 144)  41472       stage3/block8/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/bn_gconv_1 (Batch (None, 14, 14, 144)  576         stage4/block1/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/relu_gconv_1 (Act (None, 14, 14, 144)  0           stage4/block1/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/channel_shuffle ( (None, 14, 14, 144)  0           stage4/block1/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_dwconv_1 (Dep (None, 7, 7, 144)    1296        stage4/block1/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/bn_dwconv_1 (Batc (None, 7, 7, 144)    576         stage4/block1/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_2 (Conv (None, 7, 7, 288)    41472       stage4/block1/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/bn_gconv_2 (Batch (None, 7, 7, 288)    1152        stage4/block1/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/avg_pool (Average (None, 7, 7, 288)    0           stage3/block8/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/concat (Concatena (None, 7, 7, 576)    0           stage4/block1/bn_gconv_2[0][0]   \n",
      "                                                                 stage4/block1/avg_pool[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/relu_out (Activat (None, 7, 7, 576)    0           stage4/block1/concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_1 (Conv (None, 7, 7, 144)    82944       stage4/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/bn_gconv_1 (Batch (None, 7, 7, 144)    576         stage4/block2/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/relu_gconv_1 (Act (None, 7, 7, 144)    0           stage4/block2/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/channel_shuffle ( (None, 7, 7, 144)    0           stage4/block2/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_dwconv_1 (Dep (None, 7, 7, 144)    1296        stage4/block2/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/bn_dwconv_1 (Batc (None, 7, 7, 144)    576         stage4/block2/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_2 (Conv (None, 7, 7, 576)    82944       stage4/block2/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/bn_gconv_2 (Batch (None, 7, 7, 576)    2304        stage4/block2/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/add (Add)         (None, 7, 7, 576)    0           stage4/block2/bn_gconv_2[0][0]   \n",
      "                                                                 stage4/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/relu_out (Activat (None, 7, 7, 576)    0           stage4/block2/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_1 (Conv (None, 7, 7, 144)    82944       stage4/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/bn_gconv_1 (Batch (None, 7, 7, 144)    576         stage4/block3/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/relu_gconv_1 (Act (None, 7, 7, 144)    0           stage4/block3/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/channel_shuffle ( (None, 7, 7, 144)    0           stage4/block3/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_dwconv_1 (Dep (None, 7, 7, 144)    1296        stage4/block3/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/bn_dwconv_1 (Batc (None, 7, 7, 144)    576         stage4/block3/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_2 (Conv (None, 7, 7, 576)    82944       stage4/block3/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/bn_gconv_2 (Batch (None, 7, 7, 576)    2304        stage4/block3/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/add (Add)         (None, 7, 7, 576)    0           stage4/block3/bn_gconv_2[0][0]   \n",
      "                                                                 stage4/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/relu_out (Activat (None, 7, 7, 576)    0           stage4/block3/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_1 (Conv (None, 7, 7, 144)    82944       stage4/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/bn_gconv_1 (Batch (None, 7, 7, 144)    576         stage4/block4/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/relu_gconv_1 (Act (None, 7, 7, 144)    0           stage4/block4/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/channel_shuffle ( (None, 7, 7, 144)    0           stage4/block4/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_dwconv_1 (Dep (None, 7, 7, 144)    1296        stage4/block4/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/bn_dwconv_1 (Batc (None, 7, 7, 144)    576         stage4/block4/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_2 (Conv (None, 7, 7, 576)    82944       stage4/block4/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/bn_gconv_2 (Batch (None, 7, 7, 576)    2304        stage4/block4/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/add (Add)         (None, 7, 7, 576)    0           stage4/block4/bn_gconv_2[0][0]   \n",
      "                                                                 stage4/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/relu_out (Activat (None, 7, 7, 576)    0           stage4/block4/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_pool (GlobalMaxPooling2D (None, 576)          0           stage4/block4/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "fc (Dense)                      (None, 4)            2308        global_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 4)            0           fc[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 971,836\n",
      "Trainable params: 957,196\n",
      "Non-trainable params: 14,640\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"ShuffleNet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
